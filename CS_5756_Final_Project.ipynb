{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91r580K0p6en",
        "outputId": "a77a2902-95ac-4f95-a365-b70a18dde9f7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if USING_COLAB:\n",
        "    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "    !pip install -U renderlab\n",
        "    !pip install -U colabgymrender\n",
        "    !pip install -U moviepy==0.2.3.5\n",
        "    !pip install imageio==2.4.1\n",
        "    !pip install --upgrade AutoROM\n",
        "    !AutoROM --accept-license\n",
        "    !pip install gymnasium\n",
        "    !pip install flappy-bird-gymnasium\n",
        "    !pip install gym[classic_control] > /dev/null 2>&1\n",
        "    !pip install stable_baselines3\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "import flappy_bird_gymnasium\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "seed = 24\n",
        "data_seed = 700"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwT7b0avU6T8"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIy9ryzcUJ8a"
      },
      "outputs": [],
      "source": [
        "def reseed(seed, env=None):\n",
        "    '''\n",
        "        Sets the seed for reproducibility\n",
        "\n",
        "        When @param env is provided, also sets the\n",
        "        random number generataor of the gym environment\n",
        "        to this particular seed\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if env is not None:\n",
        "        env.unwrapped._np_random = gym.utils.seeding.np_random(seed)[0]\n",
        "\n",
        "reseed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRDcjinISilN"
      },
      "outputs": [],
      "source": [
        "def visualize(env_name='FlappyBird-v0', algorithm=None, video_name=\"test\", env_args={}):\n",
        "    \"\"\"Visualize a policy network for a given algorithm on a single episode\n",
        "\n",
        "        Args:\n",
        "            env_name: Name of the gym environment to roll out `algorithm` in, it will be instantiated using gym.make or make_vec_env\n",
        "            algorithm (PPOActor): Actor whose policy network will be rolled out for the episode. If\n",
        "            no algorithm is passed in, a random policy will be visualized.\n",
        "            video_name (str): Name for the mp4 file of the episode that will be saved (omit .mp4). Only used\n",
        "            when running on local machine.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_action(obs):\n",
        "        if not algorithm:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            return algorithm.select_action(obs)\n",
        "\n",
        "    if USING_COLAB:\n",
        "        from renderlab import RenderFrame\n",
        "\n",
        "        directory = './video'\n",
        "        env_args['render_mode'] = 'rgb_array'\n",
        "        env = gym.make(env_name, **env_args)\n",
        "        env = RenderFrame(env, directory)\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        for i in range(1000):\n",
        "            action = get_action(obs)\n",
        "            obs, reward, done, truncate, info = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        env.play()\n",
        "    else:\n",
        "        import cv2\n",
        "\n",
        "        video = cv2.VideoWriter(f\"{video_name}.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), 24, (600,400))\n",
        "\n",
        "        env_args['render_mode'] = 'rgb_array'\n",
        "        env = gym.make(env_name, **env_args)\n",
        "        obs, info = env.reset()\n",
        "\n",
        "        for i in range(500):\n",
        "            action = get_action(obs)\n",
        "            res = env.step(action)\n",
        "            obs, reward, done, truncate, info = res\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            im = env.render()\n",
        "            im = im[:,:,::-1]\n",
        "\n",
        "            video.write(im)\n",
        "\n",
        "        video.release()\n",
        "        env.close()\n",
        "        print(f\"Video saved as {video_name}.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "dlZoWmUoSx85",
        "outputId": "ce8f11c7-5d9a-4fb5-ee69-3e0920e99a6a"
      },
      "outputs": [],
      "source": [
        "flappy_env_name = \"FlappyBird-v0\"\n",
        "visualize(env_name=flappy_env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6HYMSDUUMcT"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(actor, environment, num_episodes=100, progress=True):\n",
        "    '''\n",
        "        Returns the mean trajectory reward of rolling out `actor` on `environment\n",
        "\n",
        "        Parameters\n",
        "        - actor: PPOActor instance, defined in Part 1\n",
        "        - environment: classstable_baselines3.common.vec_env.VecEnv instance\n",
        "        - num_episodes: total number of trajectories to collect and average over\n",
        "    '''\n",
        "    total_rew = 0\n",
        "\n",
        "    iterate = (trange(num_episodes) if progress else range(num_episodes))\n",
        "    for _ in iterate:\n",
        "        obs = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = actor.select_action(obs)\n",
        "\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            total_rew += reward\n",
        "\n",
        "            obs = next_obs\n",
        "\n",
        "    return (total_rew / num_episodes).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuG40N5zU9Db"
      },
      "source": [
        "## PPO algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ny6zoQRXzio"
      },
      "outputs": [],
      "source": [
        "# Dependencies:\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.vec_env.base_vec_env import VecEnv\n",
        "\n",
        "hyperparameters = {\n",
        "    \"n_steps\": 10000,\n",
        "    \"policy_kwargs\": {\n",
        "        \"net_arch\": {\n",
        "            \"pi\": [32, 32],\n",
        "            \"vf\": [32, 32],\n",
        "            \"activation_fn\": \"tanh\",\n",
        "        }\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZPHPYK6X0pt",
        "outputId": "a5fc6f84-8233-4a08-da31-c8cbc1f2c5df"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "real_vec_env_3 = make_vec_env(flappy_env_name, n_envs = 3)\n",
        "real_vec_env_1 = make_vec_env(flappy_env_name, n_envs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpumxWbaYTxY"
      },
      "outputs": [],
      "source": [
        "class PPOActor():\n",
        "    def __init__(self, ckpt: str=None, environment: VecEnv=None, model=None):\n",
        "        '''\n",
        "          Requires environment to be a 1-vectorized environment\n",
        "\n",
        "          The `ckpt` is a .zip file path that leads to the checkpoint you want\n",
        "          to use for this particular actor.\n",
        "\n",
        "          If the `model` variable is provided, then this constructor will store\n",
        "          that as the internal representing model instead of loading one from the\n",
        "          checkpoint path\n",
        "\n",
        "        '''\n",
        "        assert ckpt is not None or model is not None\n",
        "        if model is not None:\n",
        "            self.model = model\n",
        "            return\n",
        "\n",
        "        # TODO: MODIFY\n",
        "        self.model = PPO.load(ckpt, env = environment)\n",
        "        # End TODO\n",
        "\n",
        "\n",
        "    def select_action(self, obs):\n",
        "        '''\n",
        "          Gives the action prediction of this particular actor\n",
        "        '''\n",
        "        # TODO:\n",
        "        action, states = self.model.predict(obs)\n",
        "        return action\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4UJZ4e8Ybrn"
      },
      "outputs": [],
      "source": [
        "class PPOCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0, save_path='default', eval_env=None):\n",
        "        super(PPOCallback, self).__init__(verbose)\n",
        "        self.rewards = []\n",
        "\n",
        "        self.save_freq = 120000\n",
        "        self.min_reward = -np.inf\n",
        "        self.actor = None\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.eval_steps = []\n",
        "        self.eval_rewards = []\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "\n",
        "        self.actor = PPOActor(model=self.model)\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "\n",
        "        episode_info = self.model.ep_info_buffer\n",
        "        rewards = [ep_info['r'] for ep_info in episode_info]\n",
        "        mean_rewards = np.mean(rewards)\n",
        "\n",
        "        self.rewards.append(mean_rewards)\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        if self.eval_env is None:\n",
        "            return True\n",
        "\n",
        "        if self.num_timesteps % self.save_freq == 0 and self.num_timesteps != 0:\n",
        "            mean_reward = evaluate_policy(self.actor, environment=self.eval_env, num_episodes=20)\n",
        "            print(f'evaluating {self.num_timesteps=}, {mean_reward=}=======')\n",
        "\n",
        "            self.eval_steps.append(self.num_timesteps)\n",
        "            self.eval_rewards.append(mean_reward)\n",
        "            if mean_reward > self.min_reward:\n",
        "                self.min_reward = mean_reward\n",
        "                self.model.save(self.save_path)\n",
        "                print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        plt.plot(self.eval_steps, self.eval_rewards, c='red')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Rewards over Episodes')\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D1FLj5EUYU1l",
        "outputId": "e40fa8fd-e40d-4d16-902a-1345e5e1daef"
      },
      "outputs": [],
      "source": [
        "ckpt_path = 'expert'\n",
        "total_steps = 1500000\n",
        "\n",
        "reseed(seed)\n",
        "expert_callback = PPOCallback(save_path=ckpt_path, eval_env=real_vec_env_1)\n",
        "\n",
        "# TODO\n",
        "expert = PPO('MlpPolicy', env = real_vec_env_3, **hyperparameters)\n",
        "expert.learn(total_timesteps=total_steps, callback=expert_callback)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJRq9rmDa-Bw",
        "outputId": "eb89262a-2caf-4035-b240-5239dde9277a"
      },
      "outputs": [],
      "source": [
        "expert_actor = PPOActor(ckpt=\"expert.zip\", environment = real_vec_env_1)\n",
        "reward_ppo = evaluate_policy(expert_actor, real_vec_env_1)\n",
        "print(reward_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "xJAvvU-TbBzG",
        "outputId": "037f3800-08c7-4e35-b852-a56b14aed9be"
      },
      "outputs": [],
      "source": [
        "visualize(algorithm=expert_actor, video_name='expert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZgH7jvjofIQ"
      },
      "source": [
        "## A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWlpvr6J3p8J"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFCkHf28AZwX"
      },
      "outputs": [],
      "source": [
        "class A2CActor():\n",
        "    def __init__(self, ckpt: str=None, environment: VecEnv=None, model=None):\n",
        "        '''\n",
        "          Requires environment to be a 1-vectorized environment\n",
        "\n",
        "          The `ckpt` is a .zip file path that leads to the checkpoint you want\n",
        "          to use for this particular actor.\n",
        "\n",
        "          If the `model` variable is provided, then this constructor will store\n",
        "          that as the internal representing model instead of loading one from the\n",
        "          checkpoint path\n",
        "\n",
        "        '''\n",
        "        assert ckpt is not None or model is not None\n",
        "        if model is not None:\n",
        "            self.model = model\n",
        "            return\n",
        "\n",
        "        # TODO: MODIFY\n",
        "        self.model = A2C.load(ckpt, env = environment)\n",
        "        # End TODO\n",
        "\n",
        "\n",
        "    def select_action(self, obs):\n",
        "        '''\n",
        "          Gives the action prediction of this particular actor\n",
        "        '''\n",
        "        # TODO:\n",
        "        action, states = self.model.predict(obs)\n",
        "        return action\n",
        "        # END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF5GLS6nAjVx"
      },
      "outputs": [],
      "source": [
        "class A2CCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0, save_path='default', eval_env=None):\n",
        "        super(A2CCallback, self).__init__(verbose)\n",
        "        self.rewards = []\n",
        "\n",
        "        self.save_freq = 10000\n",
        "        self.min_reward = -np.inf\n",
        "        self.actor = None\n",
        "        self.eval_env = eval_env\n",
        "\n",
        "        self.save_path = save_path\n",
        "\n",
        "        self.eval_steps = []\n",
        "        self.eval_rewards = []\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "\n",
        "        self.actor = A2CActor(model=self.model)\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "\n",
        "        episode_info = self.model.ep_info_buffer\n",
        "        rewards = [ep_info['r'] for ep_info in episode_info]\n",
        "        mean_rewards = np.mean(rewards)\n",
        "\n",
        "        self.rewards.append(mean_rewards)\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        if self.eval_env is None:\n",
        "            return True\n",
        "\n",
        "        if self.num_timesteps % self.save_freq == 0 and self.num_timesteps != 0:\n",
        "            mean_reward = evaluate_policy(self.actor, environment=self.eval_env, num_episodes=20)\n",
        "            print(f'evaluating {self.num_timesteps=}, {mean_reward=}=======')\n",
        "\n",
        "            self.eval_steps.append(self.num_timesteps)\n",
        "            self.eval_rewards.append(mean_reward)\n",
        "            if mean_reward > self.min_reward:\n",
        "                self.min_reward = mean_reward\n",
        "                self.model.save(self.save_path)\n",
        "                print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        print(f'model saved on eval reward: {self.min_reward}')\n",
        "\n",
        "        plt.plot(self.eval_steps, self.eval_rewards, c='red')\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards')\n",
        "        plt.title('Rewards over Episodes')\n",
        "\n",
        "        plt.show()\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6xIhmJNFA7J",
        "outputId": "f1a0f929-ddb4-42a7-8bef-c53f70916f3b"
      },
      "outputs": [],
      "source": [
        "ckpt_path = 'expert'\n",
        "total_steps = 1500000\n",
        "reseed(seed)\n",
        "expert_callback = A2CCallback(save_path=ckpt_path, eval_env=real_vec_env_1)\n",
        "\n",
        "# TODO\n",
        "expert = A2C('MlpPolicy', env = real_vec_env_3, **hyperparameters)\n",
        "expert.learn(total_timesteps=total_steps, callback=expert_callback)\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fquq2y1VKdhk",
        "outputId": "766684e7-e198-4768-d1b8-a4fb9b407eb7"
      },
      "outputs": [],
      "source": [
        "expert_actor_a2c = A2CActor(ckpt=\"expert.zip\", environment = real_vec_env_1)\n",
        "reward_a2c = evaluate_policy(expert_actor_a2c, real_vec_env_1)\n",
        "print(reward_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "oxd0vbAuKkmS",
        "outputId": "a844c117-b32e-435e-eb9c-ed169b03ff9d"
      },
      "outputs": [],
      "source": [
        "visualize(algorithm=expert_actor_a2c, video_name='expert_a2c')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
